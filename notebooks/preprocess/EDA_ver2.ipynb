{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n"
   ],
   "id": "7311d2d8392f2032"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n"
   ],
   "id": "7a9f0b96dcbcfb45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# '[.]'을 '.'으로 복구\n",
    "train['URL'] = train['URL'].str.replace(r'\\[\\.\\]', '.', regex=True)\n",
    "test['URL'] = test['URL'].str.replace(r'\\[\\.\\]', '.', regex=True)\n",
    "\n"
   ],
   "id": "c22081973988bd62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 엔트로피 계산 함수 정의\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"\n",
    "    주어진 문자열의 엔트로피를 계산합니다.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): 엔트로피를 계산할 문자열\n",
    "\n",
    "    Returns:\n",
    "    float: 계산된 엔트로피 값\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    # 각 문자의 빈도수 계산\n",
    "    counter = Counter(text)\n",
    "\n",
    "    # 문자열 길이\n",
    "    length = len(text)\n",
    "\n",
    "    # 각 문자의 확률 계산 후 엔트로피 계산\n",
    "    entropy = 0\n",
    "    for count in counter.values():\n",
    "        probability = count / length\n",
    "        entropy -= probability * math.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n"
   ],
   "id": "3108d7bdbcd64851"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# URL 구조 세분화 전처리 과정 추가\n",
    "def extract_url_features(df):\n",
    "    # URL 기본 구성요소 추출: scheme, netloc, path, params, query, fragment\n",
    "    parsed_urls = df['URL'].apply(urlparse)\n",
    "    df['scheme'] = parsed_urls.apply(lambda x: x.scheme)\n",
    "    df['netloc'] = parsed_urls.apply(lambda x: x.netloc)\n",
    "    df['path'] = parsed_urls.apply(lambda x: x.path)\n",
    "    df['params'] = parsed_urls.apply(lambda x: x.params)\n",
    "    df['query'] = parsed_urls.apply(lambda x: x.query)\n",
    "    df['fragment'] = parsed_urls.apply(lambda x: x.fragment)\n",
    "\n",
    "    # tldextract를 이용하여 도메인 세분화: 서브도메인, 도메인, TLD 추출\n",
    "    extracted = df['URL'].apply(tldextract.extract)\n",
    "    df['subdomain_text'] = extracted.apply(lambda x: x.subdomain)\n",
    "    df['domain_text'] = extracted.apply(lambda x: x.domain)\n",
    "    df['suffix_text'] = extracted.apply(lambda x: x.suffix)\n",
    "\n",
    "    # 추가 피처: 경로 내 세그먼트 개수, 쿼리 파라미터 개수\n",
    "    df['path_segment_count'] = df['path'].apply(lambda x: len([seg for seg in x.split('/') if seg]))\n",
    "    df['query_param_count'] = df['query'].apply(lambda x: len(x.split('&')) if x else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# train, test 데이터에 URL 구조 세분화 적용\n",
    "train = extract_url_features(train)\n",
    "test = extract_url_features(test)\n"
   ],
   "id": "179c655f434534fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## 새로운 변수 생성\n",
    "# URL 길이\n",
    "train['length'] = train['URL'].str.len()\n",
    "test['length'] = test['URL'].str.len()\n",
    "\n",
    "# 서브도메인 개수 (기존 방식)\n",
    "train['subdomain_count'] = train['URL'].str.split('.').apply(lambda x: len(x) - 2)\n",
    "test['subdomain_count'] = test['URL'].str.split('.').apply(lambda x: len(x) - 2)\n",
    "\n",
    "# 특수 문자('-', '_', '/') 개수\n",
    "train['special_char_count'] = train['URL'].apply(lambda x: sum(1 for c in x if c in '-_/'))\n",
    "test['special_char_count'] = test['URL'].apply(lambda x: sum(1 for c in x if c in '-_/'))\n",
    "\n",
    "# 디지털 문자 관련\n",
    "train['digit_count'] = train['URL'].str.count(r'\\d')\n",
    "test['digit_count'] = test['URL'].str.count(r'\\d')\n",
    "train['digit_ratio'] = train['digit_count'] / train['length']\n",
    "test['digit_ratio'] = test['digit_count'] / test['length']\n",
    "\n",
    "# 대문자 관련\n",
    "train['uppercase_count'] = train['URL'].str.count(r'[A-Z]')\n",
    "test['uppercase_count'] = test['URL'].str.count(r'[A-Z]')\n",
    "train['uppercase_ratio'] = train['uppercase_count'] / train['length']\n",
    "test['uppercase_ratio'] = test['uppercase_count'] / test['length']\n",
    "\n",
    "# 추가 특수문자\n",
    "train['abnormal_chars'] = train['URL'].str.count(r'[^a-zA-Z0-9\\-\\./_]')\n",
    "test['abnormal_chars'] = test['URL'].str.count(r'[^a-zA-Z0-9\\-\\./_]')\n",
    "train['dots_count'] = train['URL'].str.count(r'\\.')\n",
    "test['dots_count'] = test['URL'].str.count(r'\\.')\n",
    "\n",
    "# URL 구조 관련 기존 피처\n",
    "train['path_length'] = train['URL'].apply(lambda x: len(x.split('/')[-1]) if '/' in x else 0)\n",
    "test['path_length'] = test['URL'].apply(lambda x: len(x.split('/')[-1]) if '/' in x else 0)\n",
    "\n",
    "train['query_count'] = train['URL'].str.count(r'\\?')\n",
    "test['query_count'] = test['URL'].str.count(r'\\?')\n",
    "\n",
    "train['and_count'] = train['URL'].str.count(r'\\&')\n",
    "test['and_count'] = test['URL'].str.count(r'\\&')\n"
   ],
   "id": "a96ffd4f3dd684f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 엔트로피 특성 추가\n",
    "# 전체 URL의 엔트로피\n",
    "train['url_entropy'] = train['URL'].apply(calculate_entropy)\n",
    "test['url_entropy'] = test['URL'].apply(calculate_entropy)\n",
    "\n",
    "# 도메인 부분의 엔트로피\n",
    "train['domain_entropy'] = train['netloc'].apply(calculate_entropy)\n",
    "test['domain_entropy'] = test['netloc'].apply(calculate_entropy)\n",
    "\n",
    "# 경로 부분의 엔트로피\n",
    "train['path_entropy'] = train['path'].apply(calculate_entropy)\n",
    "test['path_entropy'] = test['path'].apply(calculate_entropy)\n",
    "\n",
    "# 쿼리 부분의 엔트로피\n",
    "train['query_entropy'] = train['query'].apply(calculate_entropy)\n",
    "test['query_entropy'] = test['query'].apply(calculate_entropy)\n",
    "\n",
    "# 서브도메인 부분의 엔트로피\n",
    "train['subdomain_entropy'] = train['subdomain_text'].apply(calculate_entropy)\n",
    "test['subdomain_entropy'] = test['subdomain_text'].apply(calculate_entropy)\n",
    "\n",
    "# TLD 부분의 엔트로피\n",
    "train['tld_entropy'] = train['suffix_text'].apply(calculate_entropy)\n",
    "test['tld_entropy'] = test['suffix_text'].apply(calculate_entropy)\n",
    "\n",
    "# 최상위 도메인의 엔트로피\n",
    "train['main_domain_entropy'] = train['domain_text'].apply(calculate_entropy)\n",
    "test['main_domain_entropy'] = test['domain_text'].apply(calculate_entropy)\n"
   ],
   "id": "bcdf8d1a2cd2a69e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 상관계수 계산 (새로 추가된 엔트로피 특성 포함)\n",
    "feature_cols = ['length', 'subdomain_count', 'special_char_count',\n",
    "                'digit_count', 'digit_ratio', 'uppercase_count', 'uppercase_ratio',\n",
    "                'abnormal_chars', 'dots_count', 'path_length', 'query_count', 'and_count',\n",
    "                'url_entropy', 'domain_entropy', 'path_entropy', 'query_entropy',\n",
    "                'subdomain_entropy', 'tld_entropy', 'main_domain_entropy']\n",
    "\n",
    "correlation_matrix = train[feature_cols + ['label']].corr()\n",
    "\n",
    "# label과의 상관관계 확인\n",
    "label_corr = correlation_matrix['label'].abs().sort_values(ascending=False)\n",
    "print(\"\\n특성과 label의 상관관계 (절대값 기준 내림차순):\")\n",
    "print(label_corr)\n",
    "\n",
    "# 상관계수 0.3 이상인 특성 선택\n",
    "selected_features = label_corr[label_corr >= 0.3].index.tolist()\n",
    "selected_features.remove('label')  # label 제외\n",
    "print(\"\\n선택된 특성:\", selected_features)\n",
    "\n",
    "# 최종 데이터셋 생성\n",
    "final_train = train[['ID', 'URL', 'label'] + selected_features]\n",
    "final_test = test[['ID', 'URL'] + selected_features]\n",
    "\n",
    "print(\"\\n최종 학습 데이터 shape:\", final_train.shape)\n",
    "print(\"최종 테스트 데이터 shape:\", final_test.shape)\n"
   ],
   "id": "cbacfc2b9c8f844e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 엔트로피 특성의 분포 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, feature in enumerate(['url_entropy', 'domain_entropy', 'path_entropy',\n",
    "                             'query_entropy', 'subdomain_entropy', 'main_domain_entropy']):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.histplot(data=train, x=feature, hue='label', kde=True, bins=30)\n",
    "    plt.title(f'{feature} 분포')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('빈도')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/entropy_features_distribution.png')\n",
    "plt.show()\n"
   ],
   "id": "e3ac4db7eceb6a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 엔트로피 특성과 다른 주요 특성 간의 산점도\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 상위 4개 특성과 url_entropy의 관계\n",
    "top_features = label_corr.iloc[1:5].index.tolist()  # label 제외한 상위 4개 특성\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.scatterplot(data=train, x=feature, y='url_entropy', hue='label', alpha=0.7)\n",
    "    plt.title(f'{feature} vs url_entropy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/entropy_correlation_scatter.png')\n",
    "plt.show()\n"
   ],
   "id": "606114ad3adad8eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_train.to_csv('../data/preprocessed_data/final_train_ver2.csv', index=False)\n",
    "final_test.to_csv('../data/preprocessed_data/final_test_ver2.csv', index=False)"
   ],
   "id": "11e323104485f60b"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
